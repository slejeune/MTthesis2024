{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "355640ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the pre-trained model\n",
    "# !wget https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79930709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import fasttext\n",
    "import os\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ccf166f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained fastText language identification model\n",
    "model_path = 'lid.176.bin'  # Path to the fastText model\n",
    "fasttext_model = fasttext.load_model(model_path)\n",
    "\n",
    "# Function to predict the language of a given text\n",
    "def predict_language(text, model):\n",
    "    predictions = model.predict(text)\n",
    "    language = predictions[0][0].replace('__label__', '')  # Clean up the output\n",
    "    confidence = predictions[1][0]  # Confidence score\n",
    "    return language, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76b53842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Language: fr\n",
      "Confidence Score: 0.9996371865272522\n"
     ]
    }
   ],
   "source": [
    "# Example text for testing\n",
    "text = \"Ceci est un texte en fran√ßais.\"\n",
    "predicted_language, confidence = predict_language(text, fasttext_model)\n",
    "\n",
    "print(f\"Predicted Language: {predicted_language}\")\n",
    "print(f\"Confidence Score: {confidence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f3e8f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gathering data for ALL languages\n",
    "\n",
    "# FLORES dev set should be fine for this?\n",
    "datapath = '/Users/Suzenator/Documents/Uni/M4/MThesis/Data/flores_subset/dev/'\n",
    "output_file = 'fine_tune_data.txt'\n",
    "\n",
    "# Mapping languages to fastText codes\n",
    "langs = {\n",
    "    'Czech': 'cs',\n",
    "    'Welsh': 'cy',\n",
    "    'German': 'de',\n",
    "    'French': 'fr',\n",
    "    'Irish': 'ga',\n",
    "    'Igbo': 'ig',\n",
    "    'Japanese': 'ja',\n",
    "    'Limburgish': 'li',\n",
    "    'Luxembourgish': 'lb',\n",
    "    'Dutch': 'nl',\n",
    "    'Nepali': 'ne',\n",
    "    'Punjabi': 'pa',\n",
    "    'Russian': 'ru',\n",
    "    'Sango': 'sg',\n",
    "    'Tagalog': 'tl',\n",
    "    'Chinese': 'zh'\n",
    "}\n",
    "\n",
    "# Mapping file names to fastText language codes\n",
    "files = [\n",
    "    'ces_Latn.dev', 'cym_Latn.dev', 'deu_Latn.dev', 'fra_Latn.dev', \n",
    "    'gle_Latn.dev', 'ibo_Latn.dev', 'jpn_Jpan.dev', 'lim_Latn.dev',\n",
    "    'ltz_Latn.dev', 'nld_Latn.dev', 'npi_Deva.dev', 'pan_Guru.dev',\n",
    "    'rus_Cyrl.dev', 'sag_Latn.dev', 'tgl_Latn.dev', 'zho_Hans.dev'\n",
    "]\n",
    "\n",
    "file_to_lang_code = {\n",
    "    'ces_Latn.dev': 'cs',\n",
    "    'cym_Latn.dev': 'cy',\n",
    "    'deu_Latn.dev': 'de',\n",
    "    'fra_Latn.dev': 'fr',\n",
    "    'gle_Latn.dev': 'ga',\n",
    "    'ibo_Latn.dev': 'ig',\n",
    "    'jpn_Jpan.dev': 'ja',\n",
    "    'lim_Latn.dev': 'li',\n",
    "    'ltz_Latn.dev': 'lb',\n",
    "    'nld_Latn.dev': 'nl',\n",
    "    'npi_Deva.dev': 'ne',\n",
    "    'pan_Guru.dev': 'pa',\n",
    "    'rus_Cyrl.dev': 'ru',\n",
    "    'sag_Latn.dev': 'sg',\n",
    "    'tgl_Latn.dev': 'tl',\n",
    "    'zho_Hans.dev': 'zh'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37bec8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to fine_tune_data.txt\n"
     ]
    }
   ],
   "source": [
    "# # Preprocessing that data to be in the format that the FastText model wants it to be:\n",
    "# # __label__<language_code> <text>\n",
    "\n",
    "# # Initialize list to store processed lines\n",
    "# processed_data = []\n",
    "\n",
    "# # Process each file\n",
    "# for file in files:\n",
    "#     lang_code = file_to_lang_code[file]  # Get the language code\n",
    "#     file_path = os.path.join(datapath, file)\n",
    "\n",
    "#     # Read the file and process lines\n",
    "#     with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#         for line in f:\n",
    "#             line = line.strip()  # Remove any leading/trailing whitespace\n",
    "#             if line:  # Skip empty lines\n",
    "#                 # Add fastText label to each line\n",
    "#                 processed_data.append(f\"__label__{lang_code} {line}\")\n",
    "\n",
    "# # Shuffle the dataset (optional but recommended)\n",
    "# random.shuffle(processed_data)\n",
    "\n",
    "# # Write processed data to the output file\n",
    "# with open(output_file, 'w', encoding='utf-8') as f:\n",
    "#     for line in processed_data:\n",
    "#         f.write(line + '\\n')\n",
    "\n",
    "# print(f\"Processed data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bc62555",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  94873\n",
      "Number of labels: 16\n",
      "Progress: 100.0% words/sec/thread:  603640 lr:  0.000000 avg.loss:  0.138896 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "# Finetuning FastText\n",
    "# finetuned_model = fasttext.train_supervised(input=\"fine_tune_data.txt\",\n",
    "#                  epoch=1500,   \n",
    "#                  lr=0.004,      \n",
    "#                  wordNgrams=4)  \n",
    "\n",
    "# finetuned_model.save_model('fine_tuned_model.bin')\n",
    "\n",
    "finetuned_model = fasttext.load_model('fine_tuned_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "534f31a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datapath = '/Users/Suzenator/Documents/Uni/M4/MThesis/Data/flores_subset/devtest_labeled/'\n",
    "files = ['ces_Latn.devtest', 'cym_Latn.devtest', 'deu_Latn.devtest', 'fra_Latn.devtest', \n",
    "         'gle_Latn.devtest', 'ibo_Latn.devtest', 'jpn_Jpan.devtest', 'lim_Latn.devtest', \n",
    "         'ltz_Latn.devtest', 'nld_Latn.devtest', 'npi_Deva.devtest', 'pan_Guru.devtest', \n",
    "         'rus_Cyrl.devtest', 'sag_Latn.devtest', 'tgl_Latn.devtest', 'zho_Hans.devtest']\n",
    "file_to_lang_code = {\n",
    "    'ces_Latn.devtest': 'cs', 'cym_Latn.devtest': 'cy', 'deu_Latn.devtest': 'de',\n",
    "    'fra_Latn.devtest': 'fr', 'gle_Latn.devtest': 'ga', 'ibo_Latn.devtest': 'ig',\n",
    "    'jpn_Jpan.devtest': 'ja', 'lim_Latn.devtest': 'li', 'ltz_Latn.devtest': 'lb',\n",
    "    'nld_Latn.devtest': 'nl', 'npi_Deva.devtest': 'ne', 'pan_Guru.devtest': 'pa',\n",
    "    'rus_Cyrl.devtest': 'ru', 'sag_Latn.devtest': 'sg', 'tgl_Latn.devtest': 'tl',\n",
    "    'zho_Hans.devtest': 'zh'\n",
    "}\n",
    "\n",
    "# Adding Fasttext labels to the test data, ONLY DO THIS ONCE\n",
    "# for file in files:\n",
    "#     lang_code = file_to_lang_code[file]\n",
    "#     file_path = os.path.join(test_datapath, file)\n",
    "#     labeled_lines = []\n",
    "#     with open(file_path, 'r') as f:\n",
    "#         for line in f:\n",
    "#             line = line.strip()\n",
    "#             if line:\n",
    "#                 labeled_lines.append(f\"__label__{lang_code} {line}\\n\")\n",
    "#     with open(file_path, 'w') as f:\n",
    "#         f.writelines(labeled_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2f7c9021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate off-target ratio and return off-target sentences\n",
    "def get_off_target_sentences(model, file_path, correct_language_label):\n",
    "    off_target_count = 0\n",
    "    total_count = 0\n",
    "    off_target_sentences = []\n",
    "    total_confidence = []\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:  # Skip empty lines\n",
    "                total_count += 1\n",
    "                \n",
    "                # Predict language for the sentence\n",
    "                predicted_label, confidence = model.predict(line)\n",
    "                \n",
    "                # Compare the predicted label with the correct language label\n",
    "                if predicted_label[0] != correct_language_label:\n",
    "                    off_target_count += 1\n",
    "                    off_target_sentences.append(line)  # Store the off-target sentence\n",
    "                \n",
    "                total_confidence.append(confidence)\n",
    "    \n",
    "    # Calculate and return the ratio of off-target sentences and the sentences themselves\n",
    "    if total_count == 0:\n",
    "        return 0.0, off_target_sentences  # To avoid division by zero\n",
    "    \n",
    "    off_target_ratio = off_target_count / total_count\n",
    "    avg_confidence = sum(total_confidence)/len(total_confidence)\n",
    "    \n",
    "    return off_target_ratio, off_target_sentences, avg_confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ca298407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_off_target_ratio(model, alpha, scenario):\n",
    "    \n",
    "    total_ratio = []\n",
    "    total_confidence = []\n",
    "    \n",
    "    xconst_name = alpha + \"_gpt-mt\"\n",
    "    \n",
    "    # Define language pairs for evaluation\n",
    "    if scenario == \"high_high\":\n",
    "        language_pairs = [(\"nl\", \"de\"), (\"nl\", \"zh\"), (\"fr\", \"cs\"), (\"fr\", \"de\"),\n",
    "                          (\"jp\", \"zh\"), (\"jp\", \"ru\"), (\"tl\", \"ru\"), (\"tl\", \"cs\")\n",
    "                         ]\n",
    "    elif scenario == \"high_low\":\n",
    "        language_pairs = [(\"nl\", \"ig\"), (\"fr\", \"li\"), (\"jp\", \"li\"), (\"tl\", \"li\"),\n",
    "                          (\"nl\", \"ne\"), (\"fr\", \"ne\"), (\"jp\", \"ig\"), (\"tl\", \"ig\"),\n",
    "                          (\"nl\", \"cy\"), (\"fr\", \"cy\"), (\"jp\", \"cy\"), (\"tl\", \"ne\")\n",
    "                         ]\n",
    "    elif scenario == \"low_low\":\n",
    "        language_pairs = [(\"li\", \"lu\"), (\"li\", \"pa\"), (\"ig\", \"lb\"), (\"ig\", \"sg\"),\n",
    "                          (\"ne\", \"pa\"), (\"ne\", \"ga\"), (\"cy\", \"ga\"), (\"cy\", \"sg\")\n",
    "                         ]\n",
    "    else: \n",
    "        print(\"SCENARIO NOT RECOGNIZED\")\n",
    "    \n",
    "    path = '/Users/Suzenator/Documents/Uni/M4/MThesis/output/backup/total/'+scenario+'/'+xconst_name+'/zeroshot/'\n",
    "    files = os.listdir(path)\n",
    "    lang_code_to_short = {key.split('.')[0].split('_')[0]: value for key, value in file_to_lang_code.items()}\n",
    "    \n",
    "    # Reverse mapping to get the key for a given short code\n",
    "    short_to_key = {value: key.split('.')[0].split('_')[0] for key, value in file_to_lang_code.items()}\n",
    "    \n",
    "    # Generate filter list using the keys from file_to_lang_code\n",
    "    filter_list = []\n",
    "    for src, tgt in language_pairs:\n",
    "        src_key = short_to_key.get(src, None)\n",
    "        tgt_key = short_to_key.get(tgt, None)\n",
    "        if src_key and tgt_key:  # Ensure both language codes exist in the mapping\n",
    "            filter_list.append(f\"{src_key}2{tgt_key}_translation\")\n",
    "            filter_list.append(f\"{tgt_key}2{src_key}_translation\")\n",
    "\n",
    "    # Process the files\n",
    "    for file in files:\n",
    "    \n",
    "        # Check if the file is in the filter list\n",
    "        if any(f in file for f in filter_list):\n",
    "            \n",
    "            # Extract language pair from the file name\n",
    "            result = re.search(r'2(.*?)_translation', file).group(1)\n",
    "            target = lang_code_to_short.get(result, None)\n",
    "    \n",
    "            if target:  # Ensure the target language is found\n",
    "                correct_language_label = '__label__' + target\n",
    "    \n",
    "                # Calculate off-target ratio and append to total\n",
    "                off_target_ratio, off_target_sentences, confidence = get_off_target_sentences(model, path + file, correct_language_label)\n",
    "                total_ratio.append(off_target_ratio)\n",
    "                total_confidence.append(confidence)\n",
    "    \n",
    "    # Compute average off-target ratio\n",
    "    avg_ratio = sum(total_ratio) / len(total_ratio) * 100\n",
    "    avg_confidence = sum(total_confidence)/len(total_confidence) * 100\n",
    "    print(f\"Average Off-Target Ratio for {xconst_name}, {scenario}: {avg_ratio:.2f}, confidence: {avg_confidence[0]:.2f}\")\n",
    "#     print(f\"{avg_ratio:.2f} ({avg_confidence[0]:.2f}%) \")    \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "18a433ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Off-Target Ratio for 0_gpt-mt, high_high: 0.21, confidence: 98.03\n",
      "Average Off-Target Ratio for 001_gpt-mt, high_high: 0.17, confidence: 98.08\n",
      "Average Off-Target Ratio for 0025_gpt-mt, high_high: 0.21, confidence: 98.06\n",
      "Average Off-Target Ratio for 005_gpt-mt, high_high: 0.16, confidence: 98.13\n",
      "Average Off-Target Ratio for 01_gpt-mt, high_high: 0.17, confidence: 98.09\n",
      "Average Off-Target Ratio for 025_gpt-mt, high_high: 0.19, confidence: 98.13\n",
      "Average Off-Target Ratio for 05_gpt-mt, high_high: 0.26, confidence: 98.11\n",
      "Average Off-Target Ratio for 1_gpt-mt, high_high: 0.29, confidence: 98.19\n",
      "\n",
      "Average Off-Target Ratio for 0_gpt-mt, high_low: 7.90, confidence: 94.40\n",
      "Average Off-Target Ratio for 001_gpt-mt, high_low: 6.42, confidence: 94.46\n",
      "Average Off-Target Ratio for 0025_gpt-mt, high_low: 6.63, confidence: 94.50\n",
      "Average Off-Target Ratio for 005_gpt-mt, high_low: 6.36, confidence: 94.51\n",
      "Average Off-Target Ratio for 01_gpt-mt, high_low: 6.35, confidence: 94.38\n",
      "Average Off-Target Ratio for 025_gpt-mt, high_low: 6.85, confidence: 94.16\n",
      "Average Off-Target Ratio for 05_gpt-mt, high_low: 6.74, confidence: 94.42\n",
      "Average Off-Target Ratio for 1_gpt-mt, high_low: 7.01, confidence: 93.73\n",
      "\n",
      "Average Off-Target Ratio for 0_gpt-mt, low_low: 45.91, confidence: 81.27\n",
      "Average Off-Target Ratio for 001_gpt-mt, low_low: 45.37, confidence: 79.16\n",
      "Average Off-Target Ratio for 0025_gpt-mt, low_low: 45.31, confidence: 78.82\n",
      "Average Off-Target Ratio for 005_gpt-mt, low_low: 42.22, confidence: 77.67\n",
      "Average Off-Target Ratio for 01_gpt-mt, low_low: 41.33, confidence: 77.54\n",
      "Average Off-Target Ratio for 025_gpt-mt, low_low: 44.17, confidence: 77.31\n",
      "Average Off-Target Ratio for 05_gpt-mt, low_low: 41.88, confidence: 77.43\n",
      "Average Off-Target Ratio for 1_gpt-mt, low_low: 41.53, confidence: 78.67\n",
      "\n"
     ]
    }
   ],
   "source": [
    "alphas = [\"0\", \"01\"]\n",
    "alphas_full = [\"0\", \"001\", \"0025\", \"005\", \"01\", \"025\", \"05\", \"1\"]\n",
    "scenarios = [\"high_high\", \"high_low\", \"low_low\"]\n",
    "\n",
    "for s in scenarios:\n",
    "    \n",
    "    if s == \"high_high\": \n",
    "        # Use the standard model for HH\n",
    "        model = fasttext_model\n",
    "    else:\n",
    "        # HL and LL use the finetuned model since they contain languages the standard model does not recognize\n",
    "        model = finetuned_model \n",
    "        \n",
    "    for a in alphas_full:\n",
    "        print_off_target_ratio(model, a, s)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0e3a7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
